ggplot(Amtrak.df, aes(y=ridership,x=as.numeric(time(ridership.ts)))) +geom_line() +
geom_smooth(formula=y~poly(x.2),method = 'lm' ,colour = "navy", se = FALSE, na.rm = TRUE) +
xlab("Year") + ylab("Ridership (in 000s)")
ggplot(Amtrak.df, aes(y = Ridership, x = Month, group = 12)) +
geom_line() + geom_smooth(formula = y ~ poly(x, 2), method= "lm",
colour = "navy", se = FALSE, na.rm = TRUE)
#modifying x to handle appropriately
ggplot(Amtrak.df, aes(y=Ridership,x=as.numeric(time(ridership.ts)))) +geom_line() +
geom_smooth(formula=y~poly(x.2),method = 'lm' ,colour = "navy", se = FALSE, na.rm = TRUE) +
xlab("Year") + ylab("Ridership (in 000s)")
#modifying x to handle appropriately
ggplot(Amtrak.df, aes(y=Ridership,x=as.numeric(time(ridership.ts)))) +geom_line() +
geom_smooth(formula=y~poly(x,2),method = 'lm' ,colour = "navy", se = FALSE, na.rm = TRUE) +
xlab("Year") + ylab("Ridership (in 000s)")
##zoom in, monthly and annual plots
ridership.2yrs <- window(ridership.ts, start = c(1991,1), end = c(1992,12))
plot(ridership.2yrs, xlab = "Year", ylab = "Ridership (in 000s)", ylim = c(1300, 2300))
##zoom in, monthly and annual plots
ridership.2yrs <- window(ridership.ts, start = c(1991,1), end = c(1992,12))
plot(ridership.2yrs, xlab = "Year", ylab = "Ridership (in 000s)", ylim = c(1300, 2300))
monthly.ridership.ts <- tapply(ridership.ts, cycle(ridership.ts), mean)
plot(monthly.ridership.ts, xlab = "Month", ylab = "Average Ridership",
ylim = c(1300, 2300), type = "l", xaxt = 'n')
##zoom in, monthly and annual plots
ridership.2yrs <- window(ridership.ts, start = c(1991,5), end = c(1992,12))
plot(ridership.2yrs, xlab = "Year", ylab = "Ridership (in 000s)", ylim = c(1300, 2300))
##zoom in, monthly and annual plots
ridership.2yrs <- window(ridership.ts, start = c(1991,2), end = c(1992,12))
plot(ridership.2yrs, xlab = "Year", ylab = "Ridership (in 000s)", ylim = c(1300, 2300))
##zoom in, monthly and annual plots
ridership.2yrs <- window(ridership.ts, start = c(1991,2), end = c(1992,12))
plot(ridership.2yrs, xlab = "Year", ylab = "Ridership (in 000s)", ylim = c(1300, 2300))
monthly.ridership.ts <- tapply(ridership.ts, cycle(ridership.ts), mean)
##zoom in, monthly and annual plots
ridership.2yrs <- window(ridership.ts, start = c(1991,2), end = c(1992,12))
plot(ridership.2yrs, xlab = "Year", ylab = "Ridership (in 000s)", ylim = c(1300, 2300))
##zoom in, monthly and annual plots
ridership.2yrs <- window(ridership.ts, start = c(1991,1), end = c(1992,12))
plot(ridership.2yrs, xlab = "Year", ylab = "Ridership (in 000s)", ylim = c(1300, 2300))
##zoom in, monthly and annual plots
ridership.2yrs <- window(ridership.ts, start = c(1991,3), end = c(1992,12))
plot(ridership.2yrs, xlab = "Year", ylab = "Ridership (in 000s)", ylim = c(1300, 2300))
##zoom in, monthly and annual plots
ridership.2yrs <- window(ridership.ts, start = c(1991,1), end = c(1992,12))
plot(ridership.2yrs, xlab = "Year", ylab = "Ridership (in 000s)", ylim = c(1300, 2300))
# package forecast is required to evaluate performance
library(forecast)
set.seed(1)
# load file
toyota.corolla.df <- read.csv("Lecture 4/ToyotaCorolla.csv")
# load file
toyota.corolla.df <- read.csv("ToyotaCorolla.csv")
# load file
toyota.corolla.df <- read.csv("ToyotaCorolla.csv")
setwd("C:/Users/26amr/OneDrive - The University of Texas at Dallas/UTD/Fall 2024/Business ANalytics with R/model evaluation lecture 4")
# load file
toyota.corolla.df <- read.csv(" ToyotaCorolla.csv")
# load file
toyota.corolla.df <- read.csv("ToyotaCorolla.csv")
# randomly generate training and validation sets
training <- sample(toyota.corolla.df$Id, 600)
validation <- sample(setdiff(toyota.corolla.df$Id, training), 400)
toyota.corolla.df$Id
toyota.corolla.df
view(toyota.corolla.df)
view(toyota.corolla.df)
View(toyota.corolla.df)
# randomly generate training and validation sets
training <- sample(toyota.corolla.df$Id, 600)
validation <- sample(setdiff(toyota.corolla.df$Id, training), 400)
# run linear regression model
reg <- lm(Price~., data=toyota.corolla.df[,-c(1,2,8,11)], subset=training,
na.action=na.exclude)
pred_t <- predict(reg, na.action=na.pass)
pred_v <- predict(reg, newdata=toyota.corolla.df[validation,-c(1,2,8,11)],
na.action=na.pass)
## evaluate performance
# training
accuracy(pred_t, toyota.corolla.df[training,]$Price)
# validation
accuracy(pred_v, toyota.corolla.df[validation,]$Price)
pred_v <- predict(reg, newdata=toyota.corolla.df[validation,-c(1,2,8,11)],
na.action=na.pass)
## evaluate performance
# training
accuracy(pred_t, toyota.corolla.df[training,]$Price)
# validation
accuracy(pred_v, toyota.corolla.df[validation,]$Price)
# randomly generate training and validation sets
training <- sample(toyota.corolla.df$Id, 600)
validation <- sample(setdiff(toyota.corolla.df$Id, training), 400)
# run linear regression model
reg <- lm(Price~., data=toyota.corolla.df[,-c(1,2,8,11)], subset=training,
na.action=na.exclude)
pred_t <- predict(reg, na.action=na.pass)
pred_v <- predict(reg, newdata=toyota.corolla.df[validation,-c(1,2,8,11)],
na.action=na.pass)
## evaluate performance
# training
accuracy(pred_t, toyota.corolla.df[training,]$Price)
# validation
accuracy(pred_v, toyota.corolla.df[validation,]$Price)
# package forecast is required to evaluate performance
library(forecast)
set.seed(1)
# load file
toyota.corolla.df <- read.csv("ToyotaCorolla.csv")
View(toyota.corolla.df)
# randomly generate training and validation sets
training <- sample(toyota.corolla.df$Id, 600)
validation <- sample(setdiff(toyota.corolla.df$Id, training), 400)
# run linear regression model
reg <- lm(Price~., data=toyota.corolla.df[,-c(1,2,8,11)], subset=training,
na.action=na.exclude)
pred_t <- predict(reg, na.action=na.pass)
pred_v <- predict(reg, newdata=toyota.corolla.df[validation,-c(1,2,8,11)],
na.action=na.pass)
## evaluate performance
# training
accuracy(pred_t, toyota.corolla.df[training,]$Price)
# validation
accuracy(pred_v, toyota.corolla.df[validation,]$Price)
##################
# Histogram and Boxplot of prediction errors
training_data <- toyota.corolla.df[training,]
validation_data <- toyota.corolla.df[validation,]
training_data$pred_t <- pred_t
validation_data$pred_v <- pred_v
training_data$error <- training_data$Price - training_data$pred_t
validation_data$error <- validation_data$Price - validation_data$pred_v
par(mfrow=c(1,2))
hist(training_data$error)
hist(validation_data$error)
boxplot(training_data$error)
boxplot(validation_data$error)
valdata <- toyota.corolla.df[validation,]
##################
valdata
# remove missing Price data
toyota.corolla.df <-
toyota.corolla.df[!is.na(toyota.corolla.df[validation,]$Price),]
# generate random Training and Validation sets
training <- sample(toyota.corolla.df$Id, 600)
validation <- sample(toyota.corolla.df$Id, 400)
# generate random Training and Validation sets
training <- sample(toyota.corolla.df$Id, 600)
validation <- sample(toyota.corolla.df$Id, 400)
# regression model based on all numerical predictors
reg <- lm(Price~., data = toyota.corolla.df[,-c(1,2,8,11)], subset = training)
# predictions
pred_v <- predict(reg, newdata = toyota.corolla.df[validation,-c(1,2,8,11)])
# load package gains, compute gains (we will use package caret for categorical y later)
library(gains)
gain <- gains(toyota.corolla.df[validation,]$Price[!is.na(pred_v)], pred_v[!is.na(pred_v)])
# cumulative lift chart
options(scipen=999) # avoid scientific notation
# First extract non-missing prices
price <- toyota.corolla.df[validation,]$Price[!is.na(toyota.corolla.df[validation,]$Price)]
plot(c(0,gain$cume.pct.of.total*sum(price))~c(0,gain$cume.obs),
xlab="# cases", ylab="Cumulative Price", main="Lift Chart", type="l")
# baseline
lines(c(0,sum(price))~c(0,dim(toyota.corolla.df[validation,])[1]), col="gray", lty=2)
# Decile-wise lift chart
barplot(gain$mean.resp/mean(price), names.arg = gain$depth,
xlab = "Percentile", ylab = "Mean Response", main = "Decile-wise lift chart")
# cumulative lift chart
options(scipen=999) # avoid scientific notation
# First extract non-missing prices
price <- toyota.corolla.df[validation,]$Price[!is.na(toyota.corolla.df[validation,]$Price)]
plot(c(0,gain$cume.pct.of.total*sum(price))~c(0,gain$cume.obs),
xlab="# cases", ylab="Cumulative Price", main="Lift Chart", type="l")
# baseline
lines(c(0,sum(price))~c(0,dim(toyota.corolla.df[validation,])[1]), col="gray", lty=2)
# Decile-wise lift chart
barplot(gain$mean.resp/mean(price), names.arg = gain$depth,
xlab = "Percentile", ylab = "Mean Response", main = "Decile-wise lift chart")
#### Table 5.5
#install future.apply --- future.apply_1.11.0.tar.gz
library(caret)
#### Table 5.5
#install future.apply --- future.apply_1.11.0.tar.gz
library(caret, pos = 'future.apply_1.11.0.tar.gz')
library(e1071)
owner.df <- read.csv("Lecture 4/ownerExample.csv")
owner.df <- read.csv("ownerExample.csv")
owner.df$Class <- as.factor(owner.df$Class)
#ConfusionMatrix(predicted, actual)
confusionMatrix(as.factor(ifelse(owner.df$Probability>0.5, 'owner', 'nonowner')),
owner.df$Class)
confusionMatrix(as.factor(ifelse(owner.df$Probability>0.25, 'owner', 'nonowner')),
owner.df$Class)
confusionMatrix(as.factor(ifelse(owner.df$Probability>0.75, 'owner', 'nonowner')),
owner.df$Class)
ggplot(owner.df)
ggplot(owner.df$Class) +
#### Figure 5.4
# replace data.frame with your own
df <- read.csv("Lecture 4/liftExample.csv")
ggplot(owner.df$Class)
ggplot(owner.df$Class)
ggplot(owner.df)
owner.df$Class <- as.factor(owner.df$Class)
#ConfusionMatrix(predicted, actual)
confusionMatrix(as.factor(ifelse(owner.df$Probability>0.5, 'owner', 'nonowner')),
owner.df$Class)
#ConfusionMatrix(predicted, actual)
pre=confusionMatrix(as.factor(ifelse(owner.df$Probability>0.5, 'owner', 'nonowner')),
owner.df$Class)
ggplot(pre)
heatmap(pre)
pre
heatmap(pre$table)
heatmap(pre$table,Rowv = NA)
heatmap(pre$table,Rowv = NA,Colv = NA)
ggplot(pre$table)
ggplot(pre$table)
ggplot(melt(pre$table))
pre.melt=melt(pre$table)
ggplot(melt(pre$table))
ggplot(pre$table)
ggplot(pre$table)
pre.melt
pre$table
# replace data.frame with your own
df <- read.csv("liftExample.csv")
# create empty accuracy table
accT = c()
# compute accuracy per cutoff
for (cut in seq(0,1,0.1)){
cm <- confusionMatrix(as.factor(1 * (df$prob > cut)), as.factor(df$actual))
accT = c(accT, cm$overall[1])
}
# plot accuracy
plot(accT ~ seq(0,1,0.1), xlab = "Cutoff Value", ylab = "", type = "l", ylim = c(0, 1))
lines(1-accT ~ seq(0,1,0.1), type = "l", lty = 2)
legend("topright",  c("accuracy", "overall error"), lty = c(1, 2), merge = TRUE)
# create empty accuracy table
accT = c()
# compute accuracy per cutoff
for (cut in seq(0,1,0.1)){
cm <- confusionMatrix(as.factor(1 * (df$prob > cut)), as.factor(df$actual))
accT = c(accT, cm$overall[1])
}
# plot accuracy
plot(accT ~ seq(0,1,0.1), xlab = "Cutoff Value", ylab = "", type = "l", ylim = c(0, 1))
lines(1-accT ~ seq(0,1,0.1), type = "l", lty = 2)
library(pROC)
r <- roc(df$actual, df$prob)
#### Figure 5.5
install.packages(pROC)
library(pROC)
r <- roc(df$actual, df$prob)
plot.roc(r)
# compute auc
auc(r)
# first option with 'caret' library:
library(caret)
lift.example <- lift(relevel(as.factor(actual), ref="1") ~ prob, data = df)
xyplot(lift.example, plot = "gain")
library(pROC)
r <- roc(df$actual, df$prob)
plot.roc(r)
library(pROC)
#### Figure 5.5
install.packages(pROC)
library(pROC)
r <- roc(df$actual, df$prob)
plot.roc(r)
# compute auc
auc(r)
# first option with 'caret' library:
library(caret)
lift.example <- lift(relevel(as.factor(actual), ref="1") ~ prob, data = df)
xyplot(lift.example, plot = "gain")
# first option with 'caret' library:
library(caret)
lift.example <- lift(relevel(as.factor(actual), ref="1") ~ prob, data = df)
xyplot(lift.example, plot = "gain")
# Second option with 'gains' library:
library(gains)
df <- read.csv("Lecture 4/liftExample.csv")
df <- read.csv("liftExample.csv")
gain <- gains(df$actual, df$prob, groups=dim(df)[1])
plot(c(0, gain$cume.pct.of.total*sum(df$actual)) ~ c(0, gain$cume.obs),
xlab = "# cases", ylab = "Cumulative", type="l")
lines(c(0,sum(df$actual))~c(0,dim(df)[1]), col="gray", lty=2)
gain <- gains(df$actual, df$prob)
barplot(gain$mean.resp / mean(df$actual), names.arg = gain$depth, xlab = "Percentile",
ylab = "Mean Response", main = "Decile-wise lift chart")
# first option with 'caret' library:
library(caret)
lift.example <- lift(relevel(as.factor(actual), ref="1") ~ prob, data = df)
xyplot(lift.example, plot = "gain")
# load file
toyota.corolla.df <- read.csv("ToyotaCorolla.csv")
# randomly generate training and validation sets
training <- sample(toyota.corolla.df$Id, 600)
validation <- sample(setdiff(toyota.corolla.df$Id, training), 400)
# run linear regression model
reg <- lm(Price~., data=toyota.corolla.df[,-c(1,2,8,11)], subset=training,
na.action=na.exclude)
# randomly generate training and validation sets
training <- sample(toyota.corolla.df$Id, 600)
validation <- sample(setdiff(toyota.corolla.df$Id, training), 400)
# run linear regression model
reg <- lm(Price~., data=toyota.corolla.df[,-c(1,2,8,11)], subset=training,
na.action=na.exclude)
pred_t <- predict(reg, na.action=na.pass)
pred_v <- predict(reg, newdata=toyota.corolla.df[validation,-c(1,2,8,11)],
na.action=na.pass)
## evaluate performance
# training
accuracy(pred_t, toyota.corolla.df[training,]$Price)
# package forecast is required to evaluate performance
library(forecast)
## evaluate performance
# training
accuracy(pred_t, toyota.corolla.df[training,]$Price)
# validation
accuracy(pred_v, toyota.corolla.df[validation,]$Price)
setwd("C:/Users/26amr/OneDrive - The University of Texas at Dallas/UTD/Fall 2024/Business ANalytics with R/lecture 4 model evaluation")
head(car.df)
car.df=read.csv('ToyotaCorolla.csv')
head(car.df)
view(car.df)
View(car.df)
selected.var=c(3, 4, 7, 8, 9, 10, 12, 13, 14, 17, 18)
train.index=sample(c(1:1000),600)
train.index
c(1:1000)
train.index
train.df=car.df[train.index,selected.var]
train.df
view(train.df)
View(train.df)
valid.df=car.df[-train.index,selected.var]
View(valid.df)
library(forecast)
set.seed(1)
toyoto.df=read.csv('ToyotaCorolla.csv')
head(toyoto.df)
training=sample(toyoto.df$Id,600)
validation=sample(setdiff(toyoto.df$Id,training),400)
#run linear regression model
reg=lm(price~.,data=toyoto.df[,-c(1,2,8,11)], subset = training,na.action = exclude)
#run linear regression model
reg=lm(price~.,data=toyoto.df[,-c(1,2,8,11)], subset = training,na.action =na.pass
#run linear regression model
reg=lm(price~.,data=toyoto.df[,-c(1,2,8,11)], subset = training,na.action =na.pass)
#run linear regression model
reg=lm(price~.,data=toyoto.df[,-c(1,2,8,11)], subset = training,na.action =na.exclude())
#run linear regression model
reg=lm(price~.,data=toyoto.df[,-c(1,2,8,11)], subset = training,na.action =na.exclude
reg
#run linear regression model
reg=lm(price~.,data=toyoto.df[,-c(1,2,8,11)], subset = training,na.action =na.exclude
print(reg)
#run linear regression model
reg=lm(price~.,data=toyoto.df[,-c(1,2,8,11)], subset = training,na.action =na.exclude
reg
#run linear regression model
reg=lm(price~.,data=toyoto.df[,-c(1,2,8,11)], subset = training,na.action =na.exclude)
#run linear regression model
reg=lm(price~.,data=toyoto.df[,-c(1,2,8,11)], subset = training,na.action =na.exclude)
#run linear regression model
reg=lm(Price~.,data=toyoto.df[,-c(1,2,8,11)], subset = training,na.action =na.exclude)
reg
pred_t=predict(reg,na.action = na.pass)
pred_v=predict(reg,newdata = toyoto.df[validation,-c(1,2,8,11)],na.action = na.pass)
pred_v
pred_v=predict(reg,newdata = toyoto.df[validation,-c(1,2,8,11)],na.action = na.pass)
View(validation)
View(toyoto.df[validation,-c(1,2,8,11)])
accuracy(pred_t,toyoto.df[training,]$Price)
accuracy(pred_v,toyoto.df[validation,]$Price)
training_data$pred_t=pred_t
validation_data$pred_v=pred_v
training_data$error=training_data$Price-training_data$pred_t
View(training_data)
#View(training_data)
validation_data$error=validation_data$Price-validation_data$pred_v
View(validation_data)
par(mfrow=c(1,2))
hist(training_data$error)
hist(validation_data$error)
#boxplots
boxplot(training_data$error)
boxplot(validation_data$error)
#methods to remove missing values
valdata=toyota[validation]
#methods to remove missing values
valdata=toyoto.df[validation]
#methods to remove missing values
valdata=toyoto.df[,validation]
#methods to remove missing values
valdata=toyoto.df[validation,]
valdata
toyota.corolla.df=toyoto.df[!is.na(toyoto.df[validation,]$Price)]
toyota.corolla.df=toyoto.df[!is.na(toyoto.df[validation,]$Price),]
View(toyota.corolla.df)
# generate random Training and Validation sets
training <- sample(toyota.corolla.df$Id, 600)
validation <- sample(toyota.corolla.df$Id, 400)
length(training)
head(training)
head(training)
reg <- lm(Price~., data = toyota.corolla.df[,-c(1,2,8,11)], subset = training)
pred_v <- predict(reg, newdata = toyota.corolla.df[validation,-c(1,2,8,11)])
#package gains
library(gains)
gain=gains(toyota.corolla.df[validation]$Price[!is.na(pred_v)],pred_v[!is.na(pred_v)])
gain=gains(toyota.corolla.df[validation,]$Price[!is.na(pred_v)],pred_v[!is.na(pred_v)])
gain
price=toyoto.df[validation,]$Price[!is.na(toyoto.df[validation,]$Price)]
price
plot(c(0,gain$cume.pct.of.total*sum(price))-c(0,gain$cume.obs))
plot(c(0,gain$cume.pct.of.total*sum(price))~c(0,gain$cume.obs))
plot(c(0,gain$cume.pct.of.total*sum(price))~c(0,gain$cume.obs), main='Lift Chart', type='l')
# baseline
lines(c(0,sum(price))~c(0,dim(toyota.corolla.df[validation,])[1]), col="gray", lty=2)
plot(c(0,gain$cume.pct.of.total*sum(price))~c(0,gain$cume.obs),  xlab = "Percentile", ylab = "Mean Response", main='Lift Chart', type='l')
# baseline
lines(c(0,sum(price))~c(0,dim(toyota.corolla.df[validation,])[1]), col="gray", lty=2)
library(caret, pos = 'future.apply_1.11.0.tar.gz')
library(e1071)
pre=confusionMatrix(as.factor(ifelse(owner.df$Probability>0.5,'owner','nonowner')),owner.df$Class)
pre=confusionMatrix(as.factor(ifelse(owner.df$Probability>0.5,'owner','nonowner')),owner.df$Class)
pre.melt=melt(pre$table)
library(reshape)
pre.melt=melt(pre$table)
pre.melt
ggplot(pre$table) +geom_point(data=pre$table)
class(pre$table)
class(pre.melt$table)
class(pre$table)
pre$table
pre=confusionMatrix(as.factor(ifelse(owner.df$Probability>0.5,'owner','nonowner')),owner.df$Class)
pre$table
owner.df <- read.csv("ownerExample.csv")
owner.df$Class <- as.factor(owner.df$Class)
#ConfusionMatrix(predicted, actual)
pre=confusionMatrix(as.factor(ifelse(owner.df$Probability>0.5, 'owner', 'nonowner')),
owner.df$Class)
pre.melt=melt(pre$table)
library(ggplot2)
ggplot(pre$table) +geom_point(data=pre$table)
ggplot(pre$table) +geom_point(pre$table)
ggplot(pre$table) +geom_point(pre$table["owner"],pre$table["nonowner"])
class(pre$table)
ggplot(pre$table) +geom_point(as.data.frame(pre$table))
as.data.frame(pre$table)
pre=confusionMatrix(as.factor(ifelse(owner.df$Probability>0.5, 'owner', 'nonowner')),
owner.df$Class)
confusionMatrix(as.factor(ifelse(owner.df$Probability>0.25, 'owner', 'nonowner')),
owner.df$Class)
confusionMatrix(as.factor(ifelse(owner.df$Probability>0.75, 'owner', 'nonowner')),
owner.df$Class)
pre.melt=melt(pre$table)
library(ggplot2)
ggplot(pre$table) +geom_point(data = pre$table)
pre$table
as.data.frame(pre$table)
ggplot(pre$table) +geom_point(data = as.data.frame(pre$table))
pre$table
as.data.frame(pre$table)
ggplot(pre$table) +geom_point(data = as.data.frame(pre.melt))
ggplot(pre$table) +geom_point(data = pre.melt)
ggplot(pre$table) +geom_point(aes(x=pre.melt$Prediction,y=pre.melt$Reference))
# Convert the melted confusion matrix to a data frame
pre.melt <- as.data.frame(pre.melt)
# Plot the confusion matrix using ggplot2
ggplot(pre.melt, aes(x = Prediction, y = Reference, size = value, label = value)) +
geom_point() +
geom_text(vjust = -0.5) +
labs(title = "Confusion Matrix", x = "Predicted Class", y = "Actual Class") +
theme_minimal()
# Plot the confusion matrix using ggplot2
ggplot(pre.melt ) +
geom_point(aes(x = Prediction, y = Reference, size = value, label = value)) +
geom_text(vjust = -0.5) +
labs(title = "Confusion Matrix", x = "Predicted Class", y = "Actual Class") +
theme_minimal()
ggplot(pre.melt) +geom_point(data = pre$table)
ggplot(pre.melt) +geom_point(data = pre.melt)
ggplot(pre.melt)
ggplot(pre.melt)
ggplot(pre$table)
gain <- gains(df$actual, df$prob)
barplot(gain$mean.resp / mean(df$actual), names.arg = gain$depth, xlab = "Percentile",
ylab = "Mean Response", main = "Decile-wise lift chart")
# Plot the confusion matrix using ggplot2
heatmap(pre.melt)
# Plot the confusion matrix using ggplot2
heatmap(pre$table)
setwd("C:/Users/26amr/OneDrive - The University of Texas at Dallas/UTD/Fall 2024/Business ANalytics with R/lecture 5")
car.df=read.csv('ToyotaCorolla(1).csv')
head(car.df)
car.df <- car.df[1:1000, ]
# select variables for regression
selected.var <- c(3, 4, 7, 8, 9, 10, 12, 13, 14, 17, 18)
# partition data
set.seed(1)  # set seed for reproducing the partition
train.index <- sample(c(1:1000), 600)
train.df <- car.df[train.index, selected.var]
valid.df <- car.df[-train.index, selected.var]
# use lm() to run a linear regression of Price on all 11 predictors in the
# training set.
# use . after ~ to include all the remaining columns in train.df as predictors.
car.lm <- lm(Price ~ ., data = train.df)
#  use options() to ensure numbers are not displayed in scientific notation.
options(scipen = 999)
summary(car.lm)
